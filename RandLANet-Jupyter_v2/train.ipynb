{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Train文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 第1步 导入所需代码库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-08-14T02:35:07.410360800Z",
     "start_time": "2025-08-14T02:35:03.480746900Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from data_potential import CloudsDataset, ActiveLearningSampler\n",
    "from model import RandLANet\n",
    "from datetime import datetime\n",
    "import os\n",
    "from utils.metrics import compute_metrics\n",
    "from train import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 第2步 配置深度学习训练参数配"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 配置train参数\n",
    "一个epoch里面训练的总点数为：batch_size * num_points_per_step"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_dict = {\n",
    "    \"epoch\": 2,  # 训练总轮数\n",
    "    \"batch_size\": 16,  # 训练的批次大小，即包含的点云份数\n",
    "    \"train_steps\": 16,  # 这一个batch在一个epoch里训练多少次\n",
    "    \"val_steps\": 4,  # 这一个batch在一个epoch里验证多少次，可以平滑验证结果\n",
    "    \"num_points_per_step\": 10000,  # 每一份点云有多少个点\n",
    "    \"num_workers\": 1, # 设置多少线程读取内存里的点云组织成batch\n",
    "    \"lr\": 0.01,  # learning rate学习率\n",
    "    \"gpu\": 0,  # 设置使用的gpu编号，默认0号gpu\n",
    "    \"checkpoints_dir\": 'outputs/checkpoints',  # 网络模型输出目录\n",
    "    \"loggers_dir\": 'outputs/loggers',  # 日志输出目录\n",
    "    \"save_freq\": 5,  # 网络模型保存个数，只保存精度前五的网络模型，避免频繁输出开销\n",
    "    \"scheduler_gamma\": 0.95,  # 调度器对学习率的衰减率\n",
    "    \"load_model_path\": None,  # 可以加载之前训练的模型进来继续训练，输入模型的完整路径\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-14T02:35:07.426361400Z",
     "start_time": "2025-08-14T02:35:07.412361200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 配置network参数字典"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "network_dict = {\n",
    "    \"num_neighbors\": 16,  # 在进行位置编码时搜索的邻域点个数\n",
    "    \"decimation\": 4,  # 在encoding阶段下采样的比例，每次下采样采样点云个数除以4\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-14T02:35:07.455362Z",
     "start_time": "2025-08-14T02:35:07.427361900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 配置data参数字典"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    \"data_root\": \"E:/OpenGF\",  # 原始数据根目录\n",
    "    \"num_classes\": 2,  # 点云标注的类别数\n",
    "    \"grid_size\": 1.0,  # 降采样格网尺寸，1.0代表1m * 1m的范围内取一个点\n",
    "    \"HasIntensity\": True,  # 数据集是否有intensity属性，有也可以填False，表示不使用，默认使用\n",
    "    \"HasRGB\": False  # 数据集是否有RGB属性，有也可以填False，表示不使用\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-14T02:35:07.459361600Z",
     "start_time": "2025-08-14T02:35:07.444360400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 合并字典"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "merged_dict = network_dict | train_dict | data_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-14T02:35:07.475362100Z",
     "start_time": "2025-08-14T02:35:07.459361600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 第3步 创建logger和checkpoint文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-08-14T02:35:07.502360700Z",
     "start_time": "2025-08-14T02:35:07.476364300Z"
    }
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "now = datetime.now()\n",
    "logs_dir = merged_dict[\"loggers_dir\"]\n",
    "logs_dir = os.path.join(logs_dir, f\"{now.year}-{now.month}-{now.day}\")\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "checkpoint_dir = os.path.join(merged_dict[\"checkpoints_dir\"], f\"{now.year}-{now.month}-{now.day}\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 第4步 载入数据集信息\n",
    "打开记录数据集信息的metadata.json，获取class_weights信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-08-14T02:35:07.676880Z",
     "start_time": "2025-08-14T02:35:07.662880600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([96267835, 32387065])\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(merged_dict[\"data_root\"], f'processed-{merged_dict[\"grid_size\"]:.2f}', 'metadata.json')\n",
    "with open(path, 'rb') as f:\n",
    "    data_raw = json.load(f)\n",
    "data = torch.tensor(data_raw['class_weights'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 第5步 设置训练设备\n",
    "默认用GPU训练，在train.yaml文件下的gpu参数设置对应的显卡编号，默认为0号显卡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-08-14T02:35:08.560119700Z",
     "start_time": "2025-08-14T02:35:08.430023400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.cuda.set_device(int(merged_dict[\"gpu\"]))\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 第6步 创建Train/Validation Dataloder\n",
    "1. 创建dataset_train变量，将train数据导入内存\n",
    "2. 创建train_sampler变量，对dataset_train里的数据进行采样\n",
    "    - cfg.train.batch_size决定了一个batch有几份点云\n",
    "    - cfg.train.train_steps决定了一个epoch有多少个batch\n",
    "    - 设置transform=True，对train数据进行旋转、缩放、点云抖动，丰富样本\n",
    "3. 创建train_loader变量\n",
    "    - cfg.train.num_workers决定启用多少个子进程加载数据，与CPU核数有关\n",
    "    - pin_memory=True,将数据预先加载到锁页内存（Pinned Memory） 中，避免数据被交换到硬盘（虚拟内存），从而显著提升从CPU内存到GPU显存的数据传输速度\n",
    "    - drop_last=True,当数据集样本数无法被batch_size整除时，丢弃最后一个不完整的批次（样本数< batch_size），保证每个批次大小一致\n",
    "4. 创建validation dataloader同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-08-14T02:36:20.492810Z",
     "start_time": "2025-08-14T02:35:09.601570200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x00000188250F3670>\n"
     ]
    }
   ],
   "source": [
    "dataset_train = CloudsDataset(merged_dict, split='train', transform=True)\n",
    "train_sampler = ActiveLearningSampler(\n",
    "    config=merged_dict,\n",
    "    dataset=dataset_train,\n",
    "    batch_size=merged_dict[\"batch_size\"],\n",
    "    step_size=merged_dict[\"train_steps\"],\n",
    "    split='train',\n",
    "    transform=True\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_sampler,\n",
    "    batch_size=merged_dict[\"batch_size\"],\n",
    "    num_workers=merged_dict[\"num_workers\"],\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "print(train_loader)\n",
    "dataset_val = CloudsDataset(merged_dict, split='val')\n",
    "val_sampler = ActiveLearningSampler(\n",
    "    config=merged_dict,\n",
    "    dataset=dataset_val,\n",
    "    batch_size=merged_dict[\"batch_size\"],\n",
    "    step_size=merged_dict[\"val_steps\"],\n",
    "    split='val'\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_sampler,\n",
    "    batch_size=9,  # cfg.train.batch_size\n",
    "    num_workers=merged_dict[\"num_workers\"],\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 第7步 初始化RandLANet model\n",
    "- d_in 为输入特征维度，3:xyz 4:xyz+intensity 6:xyz+rgb 7:xyz+rgb+intensity\n",
    "- num_classes 点云类别数\n",
    "- num_neighbors 局部特征聚合模块邻近点搜索数\n",
    "- decimation encoder部分下采样比例，decimation=4代表，每次取1/4\n",
    "- device 决定是在GPU还是CPU，默认GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-08-14T02:36:23.260429700Z",
     "start_time": "2025-08-14T02:36:23.211429800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of class: 4\n"
     ]
    }
   ],
   "source": [
    "num_classes = merged_dict[\"num_classes\"]\n",
    "d_in = 3\n",
    "if merged_dict[\"HasRGB\"]:\n",
    "    d_in += 3\n",
    "if merged_dict[\"HasIntensity\"]:\n",
    "    d_in += 1\n",
    "print(\"num of class:\", d_in)\n",
    "model = RandLANet(\n",
    "    d_in,\n",
    "    num_classes,\n",
    "    num_neighbors=merged_dict[\"num_neighbors\"],\n",
    "    decimation=merged_dict[\"decimation\"],\n",
    "    device=device\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 第8步  计算样本权重\n",
    "以OpenGF为例，具有地面点和非地面点两类标签，势必导致样本不均衡，为了给予小样本足够关注，计算loss时赋予其较大权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-08-14T02:36:24.302446800Z",
     "start_time": "2025-08-14T02:36:24.111446500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型设备: cuda:0\n",
      "Computing weights...\ttensor([1.1560, 1.9931], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"模型设备: {next(model.parameters()).device}\")  # 输出应为 cuda:x\n",
    "\n",
    "print('Computing weights...', end='\\t')\n",
    "\n",
    "frequency = data / torch.sum(data)\n",
    "weights = 1.0 / torch.sqrt(frequency)\n",
    "weights = weights.to(torch.float).to(device)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 第9步 声明损失函数、优化器、调度器\n",
    "- 损失函数用交叉熵损失函数CrossEntropyLoss\n",
    "    - weights为样本权重\n",
    "- 优化器为Adam\n",
    "    - cfg.train.lr 为训练学习率，初始默认0.01\n",
    "- 调度器为ExponentialLR，每轮按照固定比例衰减学习率\n",
    "    -  cfg.train.scheduler_gamma 学习率衰减比例，初始为0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-08-14T02:36:24.931483Z",
     "start_time": "2025-08-14T02:36:24.911558700Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=weights).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=merged_dict[\"lr\"])\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, merged_dict[\"scheduler_gamma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 第10步 加载预训练模型\n",
    "cfg.train.load中设置预训练模型路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-08-14T02:36:25.757528500Z",
     "start_time": "2025-08-14T02:36:25.746528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "if merged_dict[\"load_model_path\"]:\n",
    "    load_path = Path(merged_dict[\"load_model_path\"])\n",
    "    path = max(list(load_path.glob('*.pth')))\n",
    "    print(f'Loading {path}...')\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "print(merged_dict[\"load_model_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 第11步 遍历epoch训练\n",
    "统计每轮的loss、accuracy、iou\n",
    "### 11.1 遍历Dataloder中的batch\n",
    "1. 一个batch一个batch的送入网络计算预测结果，与点云原始标签计算loss\n",
    "2. 根据loss沿梯度方向反向传播\n",
    "3. 收集每个batch的accuracy、iou、loss计算结果\n",
    "### 11.2 统计一个epoch的训练和验证accuracy和iou\n",
    "evaluate与train的训练过程相似，但无需梯度和反向传播，将train每个epoch导入的model进行验证，统计在验证数据集的accuracy和iou\n",
    "### 11.3 记录每个epoch的信息写入log\n",
    "### 11.4 记录每个epoch的网络模型，将验证集accuracy和miou最高的5个保存\n",
    "为防止训练中断没记录网络模型，每一个epoch都会排序出精度前5的模型，有新模型就写入硬盘，删除被替代的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-08-14T02:37:29.275479600Z",
     "start_time": "2025-08-14T02:36:26.551929300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前设备: 0 → NVIDIA GeForce RTX 3080\n",
      "====== EPOCH 1/2 ======\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000188250F3670>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xhy\\.conda\\envs\\PCDL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:751: UserWarning: Length of IterableDataset <data_potential.ActiveLearningSampler object at 0x0000018855A17580> was reported to be 4(when accessing len(dataloader)), but 5 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\xhy\\.conda\\envs\\PCDL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:751: UserWarning: Length of IterableDataset <data_potential.ActiveLearningSampler object at 0x0000018855A17580> was reported to be 4(when accessing len(dataloader)), but 6 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\xhy\\.conda\\envs\\PCDL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:751: UserWarning: Length of IterableDataset <data_potential.ActiveLearningSampler object at 0x0000018855A17580> was reported to be 4(when accessing len(dataloader)), but 7 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy      |    OA\n",
      "Training:     | 0.649\n",
      "Validation:   | 0.689\n",
      "----------------------\n",
      "MIoU          |  mIoU\n",
      "Training:     | 0.448\n",
      "Validation:   | 0.478\n",
      "Training loss: 0.6104987\n",
      "Validation loss: 0.7476123\n",
      "Time elapsed: 35 s\n",
      "\n",
      "====== EPOCH 2/2 ======\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000188250F3670>\n",
      "Accuracy      |    OA\n",
      "Training:     | 0.800\n",
      "Validation:   | 0.798\n",
      "----------------------\n",
      "MIoU          |  mIoU\n",
      "Training:     | 0.618\n",
      "Validation:   | 0.572\n",
      "Training loss: 0.5092427\n",
      "Validation loss: 0.4919806\n",
      "Time elapsed: 28 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"当前设备: {torch.cuda.current_device()} → {torch.cuda.get_device_name()}\")\n",
    "top_checkpoints = []\n",
    "epochs = int(merged_dict[\"epoch\"])\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f'====== EPOCH {epoch:d}/{epochs:d} ======')\n",
    "    t0 = time.time()\n",
    "    # Train\n",
    "    model.train()\n",
    "    # metrics\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    ious = []\n",
    "    '''11.1 遍历Dataloder中的batch'''\n",
    "    print(train_loader)\n",
    "    for batch_idx, points in enumerate(train_loader):\n",
    "        points = {k: v.to(device, non_blocking=True) for k, v in points.items()}\n",
    "        feature = points['xyz']\n",
    "        labels = points['class'].long()\n",
    "\n",
    "        if merged_dict[\"HasRGB\"]:\n",
    "            rgb = points['rgb']\n",
    "            feature = torch.cat((feature, rgb), dim=2)\n",
    "        if merged_dict[\"HasIntensity\"]:\n",
    "            intensity = points['intensity'].unsqueeze(-1)\n",
    "            feature = torch.cat((feature, intensity), dim=2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        scores = model(feature)\n",
    "        loss = criterion(scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.cpu().item())\n",
    "        if batch_idx % 10 == 0:  # 每10batch计算一次指标\n",
    "            acc, miou = compute_metrics(scores, labels, num_classes)\n",
    "            accuracies.append(acc)\n",
    "            ious.append(miou)\n",
    "\n",
    "    scheduler.step()\n",
    "    '''11.2 统计一个epoch的训练和验证accuracy和iou'''\n",
    "    accs = np.nanmean(np.array(accuracies), axis=0)\n",
    "    ious = np.nanmean(np.array(ious), axis=0)\n",
    "    val_loss, val_accs, val_ious = evaluate(\n",
    "        model,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "        num_classes,\n",
    "        merged_dict\n",
    "    )\n",
    "\n",
    "    loss_dict = {\n",
    "        'Training loss': np.mean(losses),\n",
    "        'Validation loss': val_loss\n",
    "    }\n",
    "    acc_dicts = {\n",
    "        'Training accuracy': accs,\n",
    "        'Validation accuracy': val_accs\n",
    "    }\n",
    "    iou_dicts = {\n",
    "        'Training accuracy': ious,\n",
    "        'Validation accuracy': val_ious\n",
    "    }\n",
    "\n",
    "    t1 = time.time()\n",
    "    d = t1 - t0\n",
    "\n",
    "    print('Accuracy     ', '   OA', sep=' | ')\n",
    "    print('Training:    ', *[f'{accs:.3f}' if not np.isnan(accs) else '  nan'], sep=' | ')\n",
    "    print('Validation:  ', *[f'{val_accs:.3f}' if not np.isnan(accs) else '  nan'], sep=' | ')\n",
    "    print('----------------------')\n",
    "    print('MIoU         ', ' mIoU', sep=' | ')\n",
    "    print('Training:    ', *[f'{ious:.3f}' if not np.isnan(ious) else '  nan'], sep=' | ')\n",
    "    print('Validation:  ', *[f'{val_ious:.3f}' if not np.isnan(ious) else '  nan'], sep=' | ')\n",
    "    for k, v in loss_dict.items():\n",
    "        print(f'{k}: {v:.7f}', end='\\n')\n",
    "    print('Time elapsed:', '{:.0f} s'.format(d) if d < 60 else '{:.0f} min {:02.0f} s'.format(*divmod(d, 60)))\n",
    "    print('')\n",
    "    '''11.3 记录每个epoch的信息写入log'''\n",
    "    with SummaryWriter(logs_dir) as writer:\n",
    "        # send results to tensorboard\n",
    "        writer.add_scalars('Loss', loss_dict, epoch)\n",
    "\n",
    "        writer.add_scalars('Per-class accuracy/Overall', acc_dicts, epoch)\n",
    "        writer.add_scalars('Per-class IoU/Mean IoU', iou_dicts, epoch)\n",
    "\n",
    "    '''11.4 记录每个epoch的网络模型，将验证集accuracy和miou最高的5个保存'''\n",
    "    checkpoint_path = os.path.join(checkpoint_dir,\n",
    "                                       f'checkpoint_{epoch:02d}_ACC_{val_accs:.2f}_MIOU_{val_ious:.2f}.pth')\n",
    "\n",
    "    # 更新Top-5 Checkpoints\n",
    "    if len(top_checkpoints) < 5 or val_ious > min(top_checkpoints, key=lambda x: x[0])[0]:\n",
    "        path = checkpoint_path\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'mIoU': val_ious\n",
    "        }, path)\n",
    "\n",
    "        top_checkpoints.append((val_ious, path))\n",
    "        top_checkpoints.sort(reverse=True, key=lambda x: x[0])\n",
    "        # 清理旧checkpoint\n",
    "        while len(top_checkpoints) > 5:\n",
    "            _, old_path = top_checkpoints.pop()\n",
    "            os.remove(old_path)\n",
    "\n",
    "        top_checkpoints = top_checkpoints[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 第12步 打印所花时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-08-14T02:37:29.306477100Z",
     "start_time": "2025-08-14T02:37:29.276480100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Time elapsed: 28 s.\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "d = t1 - t0\n",
    "print('Done. Time elapsed:', '{:.0f} s.'.format(d) if d < 60 else '{:.0f} min {:.0f} s.'.format(*divmod(d, 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
